version: '3'

vars:
  WAREHOUSE_DIR: "iceberg/warehouse"
  SPARK_VERSION: "3.5.5"
  KAFKA_INSTALLATION_DIR: "~/kafka-dir"
  KAFKA_TOPIC: "mentor_cruise"

tasks:

  init-iceberg-setup:
    desc: Create local warehouse directory
    cmds:
      - mkdir -p {{.WAREHOUSE_DIR}}

  setup-venv:
    desc: Setup Python virtual environment
    cmds:
      - uv venv .venv
      - source .venv/bin/activate && uv pip install -r pyproject.toml

  build-python-modules:
    desc: Build Python modules
    cmds:
      - source .venv/bin/activate && python3.12 -m pip install -e .

  setup-kafka:
    desc: Setup Kafka environment
    cmds:
      - mkdir -p {{.KAFKA_INSTALLATION_DIR}}
      - cd {{.KAFKA_INSTALLATION_DIR}} && curl -O https://dlcdn.apache.org/kafka/4.0.0/kafka_2.13-4.0.0.tgz
      - cd {{.KAFKA_INSTALLATION_DIR}} && tar -xzf kafka_2.13-4.0.0.tgz
      - cd {{.KAFKA_INSTALLATION_DIR}} && mv kafka_2.13-4.0.0 kafka

  setup-nessie-catalog:
    desc: Setup Nessie environment
    cmds:
      - docker run --rm -d --name nessie -p 19120:19120 ghcr.io/projectnessie/nessie:latest

  start-kafka:
    desc: Start Kafka service
    cmds:
      - cd {{.KAFKA_INSTALLATION_DIR}}/kafka && KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
      - cd {{.KAFKA_INSTALLATION_DIR}}/kafka && bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties
      - cd {{.KAFKA_INSTALLATION_DIR}}/kafka && bin/kafka-server-start.sh config/server.properties

  creare-kafka-topic:
    desc: Create Kafka topic
    cmds:
      - cd {{.KAFKA_INSTALLATION_DIR}}/kafka && bin/kafka-topics.sh --create --topic {{.KAFKA_TOPIC}} --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

  start-kafka-consumer:
    desc: Start Kafka consumer
    cmds:
      - cd {{.KAFKA_INSTALLATION_DIR}}/kafka && bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic {{.KAFKA_TOPIC}}

  stop-kafka:
    desc: Stop Kafka service
    cmds:
      - cd {{.KAFKA_INSTALLATION_DIR}}/kafka && bin/kafka-server-stop.sh


  run-pyspark-data-processing:
    desc: Run data generation module
    cmds:
      - source .venv/bin/activate && python -m streaming_data_pipeline.data_processing.src.main

  run-random-data-generation:
    desc: Run data generation module
    cmds:
      - source .venv/bin/activate && python -m streaming_data_pipeline.data_generation.src.main

  run-pyspark-batch-job:
    desc: Run PySpark batch job
    cmds:
      - source .venv/bin/activate && python -m streaming_data_pipeline.batch_data_pipeline.src.main